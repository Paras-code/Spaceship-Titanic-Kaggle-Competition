# -*- coding: utf-8 -*-
"""parassaini_randomforest

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/parassaini-randomforest-cbe7b08d-011e-4946-b4ec-40f20e41b3a6.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240619/auto/storage/goog4_request%26X-Goog-Date%3D20240619T072036Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9be5d73dab0a9c85af2e92cdd0415b2701020660e958a0e40881de48c300667b0da77777c463351a0d740bb61c99ac178219af0fa6203589d4365088816305244a4a568c3810dd3083ed16955b8ad9421fb58a1a1eb3d14ec447d4a8ab3a283495296f0d448563e2a48ddddff740d7fbac5ddd6eae2c567f4ffc2ba9440b73d00109a07c3a99c5e3b6c919a60d21daa4143c3a1a6cb0bf1d05962c68308d0fb7eca14c3a379a4af2c2e20bfd6654c00fe124e0f7606085086fc577c6a899159534b0529ab390f491116960d7c4b231720ac955fd2eabda96d961b4bb2523f00e6c4df4fcf8e3432e68c81ba13f019a22ba2ad4081545c582a45d07cf9917f66d
"""

import pandas as pd
import numpy as np

train_data = pd.read_csv("/kaggle/input/d/parassaini123/spaceship-titanic/train.csv")
test_data = pd.read_csv("/kaggle/input/d/parassaini123/spaceship-titanic/test.csv")

print(train_data.head())

print(train_data.count())

print(train_data.info())

print(train_data.describe())

missing_values = train_data.isnull().sum()

missing_summary = missing_values[missing_values > 0]

print("Missing values summary:")
print(missing_summary)

"""#  Data Preprocessing

**Filling missing values in Taining Data**
"""

# Fill missing values for categorical variables
train_data['HomePlanet'].fillna(train_data['HomePlanet'].mode()[0], inplace=True)
train_data['CryoSleep'].fillna(train_data['CryoSleep'].mode()[0], inplace=True)
train_data['Cabin'].fillna('Unknown/0/U', inplace=True)
train_data['Destination'].fillna(train_data['Destination'].mode()[0], inplace=True)
train_data['VIP'].fillna(train_data['VIP'].mode()[0], inplace=True)

# Fill missing values for numerical variables
train_data['Age'].fillna(train_data['Age'].median(), inplace=True)
train_data['RoomService'].fillna(train_data['RoomService'].median(), inplace=True)
train_data['FoodCourt'].fillna(train_data['FoodCourt'].median(), inplace=True)
train_data['ShoppingMall'].fillna(train_data['ShoppingMall'].median(), inplace=True)
train_data['Spa'].fillna(train_data['Spa'].median(), inplace=True)
train_data['VRDeck'].fillna(train_data['VRDeck'].median(), inplace=True)

missing_values = train_data.isnull().sum()
missing_summary = missing_values[missing_values > 0]

print("Missing values summary:")
print(missing_summary)

"""**Filling missing values in Testing data**"""

# Fill missing values for categorical variables
test_data['HomePlanet'].fillna(test_data['HomePlanet'].mode()[0], inplace=True)
test_data['CryoSleep'].fillna(test_data['CryoSleep'].mode()[0], inplace=True)
test_data['Cabin'].fillna('Unknown/0/U', inplace=True)  # 'Unknown' can be used as a placeholder
test_data['Destination'].fillna(test_data['Destination'].mode()[0], inplace=True)
test_data['VIP'].fillna(test_data['VIP'].mode()[0], inplace=True)

# Fill missing values for numerical variables using mean
test_data['Age'].fillna(test_data['Age'].median(), inplace=True)
test_data['RoomService'].fillna(test_data['RoomService'].median(), inplace=True)
test_data['FoodCourt'].fillna(test_data['FoodCourt'].median(), inplace=True)
test_data['ShoppingMall'].fillna(test_data['ShoppingMall'].median(), inplace=True)
test_data['Spa'].fillna(test_data['Spa'].median(), inplace=True)
test_data['VRDeck'].fillna(test_data['VRDeck'].median(), inplace=True)

missing_values = test_data.isnull().sum()
missing_summary = missing_values[missing_values > 0]

print("Missing values summary:")
print(missing_summary)

# Combine train and test data for encoding
combined_data = pd.concat([train_data, test_data], sort=False)

from sklearn.preprocessing import LabelEncoder

# List of categorical columns
categorical_cols = ['HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP', 'Transported']

# Apply label encoding to each categorical column on combined data
for col in categorical_cols:
    le = LabelEncoder()
    combined_data[col] = le.fit_transform(combined_data[col])

combined_data.head()

# Split the combined data back into train and test
train_data = combined_data.iloc[:len(train_data)]
test_data = combined_data.iloc[len(train_data):]

train_data.head()

# Drop columns that won't be used for prediction
train_data.drop(['Name', 'Cabin', 'PassengerId'], axis=1, inplace=True)
test_data.drop(['Name', 'Cabin'], axis=1, inplace=True)

train_data.head()

test_data.head()

"""**Feature Scaling**"""

from sklearn.preprocessing import MinMaxScaler

columns_to_scale = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']

scaler = MinMaxScaler()

# Fit and transform the scaler on training data
train_data[columns_to_scale] = scaler.fit_transform(train_data[columns_to_scale])

# Transform the scaler on test data
test_data[columns_to_scale] = scaler.transform(test_data[columns_to_scale])

train_data.head()

test_data.head()

"""# Model Training"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

X_train = train_data.drop('Transported', axis=1)
y_train = train_data['Transported']

# Ensure X_test has the same features as X_train
X_test = test_data.copy()

# Print shapes to verify
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)

from sklearn.model_selection import GridSearchCV
# Split the data into training and validation sets
X_train_split, X_valid, y_train_split, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# Initialize the Random Forest classifier
rf_model = RandomForestClassifier(random_state=42, n_jobs=-1)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')

print(y_train_split.dtype)

print(y_train.unique())

# Fit GridSearchCV to the training split
grid_search.fit(X_train_split, y_train_split)

# Get the best estimator
best_rf_model = grid_search.best_estimator_

# Make predictions on the validation set
y_pred = best_rf_model.predict(X_valid)

from sklearn.metrics import classification_report
# Evaluate the best model
accuracy = accuracy_score(y_valid, y_pred)
classification_rep = classification_report(y_valid, y_pred)

print(f'Best Parameters: {grid_search.best_params_}')
print(f'Validation Accuracy: {accuracy:.4f}')
print(f'Classification Report:\n{classification_rep}')

test_data.head()

# Exclude the unwanted columns from X_test
X_test_filtered = test_data.drop(columns=['PassengerId', 'Transported'])

# Make predictions on the filtered test set
y_test_pred = best_rf_model.predict(X_test_filtered)

X_test_filtered = test_data.drop(columns=['PassengerId', 'Transported'])

y_test_pred = best_rf_model.predict(X_test_filtered)
y_test_pred_bool = y_test_pred.astype(bool)
test_data['Transported'] = y_test_pred_bool

test_data[['PassengerId', 'Transported']].to_csv('submission.csv', index=False)

print(test_data[['PassengerId', 'Transported']])